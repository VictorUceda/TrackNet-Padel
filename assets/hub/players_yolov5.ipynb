{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Ay-jpn2wIKY_",
        "outputId": "1dc63f6a-f388-40f6-fcae-0cc83c48556d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ay-jpn2wIKY_",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_name =  'Final_PremierQatar_Tie'#'punto_qatar'# \n",
        "path = '/content/gdrive/MyDrive/PadelData/'\n",
        "path_res = '/content/gdrive/MyDrive/PadelRes/'\n",
        "point_n = 0"
      ],
      "metadata": {
        "id": "kZZQDnRDIosA"
      },
      "id": "kZZQDnRDIosA",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "chicken-bahrain",
      "metadata": {
        "id": "chicken-bahrain"
      },
      "source": [
        "### This notebook is optionally accelerated with a GPU runtime.\n",
        "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "# YOLOv5\n",
        "\n",
        "*Author: Ultralytics*\n",
        "\n",
        "**YOLOv5 in PyTorch > ONNX > CoreML > TFLite**\n",
        "\n",
        "_ | _\n",
        "- | -\n",
        "![alt](https://pytorch.org/assets/images/ultralytics_yolov5_img1.jpg) | ![alt](https://pytorch.org/assets/images/ultralytics_yolov5_img2.png)\n",
        "\n",
        "\n",
        "## Before You Start\n",
        "\n",
        "Start from a **Python>=3.8** environment with **PyTorch>=1.7** installed. To install PyTorch see [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/). To install YOLOv5 dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "double-parent",
      "metadata": {
        "id": "double-parent"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accessible-responsibility",
      "metadata": {
        "id": "accessible-responsibility"
      },
      "source": [
        "## Model Description\n",
        "\n",
        "<img width=\"800\" alt=\"YOLOv5 Model Comparison\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/model_comparison.png\">\n",
        "&nbsp;\n",
        "\n",
        "[YOLOv5](https://ultralytics.com/yolov5) üöÄ is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n",
        "\n",
        "|Model |size<br><sup>(pixels) |mAP<sup>val<br>0.5:0.95 |mAP<sup>test<br>0.5:0.95 |mAP<sup>val<br>0.5 |Speed<br><sup>V100 (ms) | |params<br><sup>(M) |FLOPS<br><sup>640 (B)\n",
        "|---   |---  |---        |---         |---             |---                |---|---              |---\n",
        "|[YOLOv5s6](https://github.com/ultralytics/yolov5/releases)   |1280 |43.3     |43.3     |61.9     |**4.3** | |12.7  |17.4\n",
        "|[YOLOv5m6](https://github.com/ultralytics/yolov5/releases)   |1280 |50.5     |50.5     |68.7     |8.4     | |35.9  |52.4\n",
        "|[YOLOv5l6](https://github.com/ultralytics/yolov5/releases)   |1280 |53.4     |53.4     |71.1     |12.3    | |77.2  |117.7\n",
        "|[YOLOv5x6](https://github.com/ultralytics/yolov5/releases)   |1280 |**54.4** |**54.4** |**72.0** |22.4    | |141.8 |222.9\n",
        "|[YOLOv5x6](https://github.com/ultralytics/yolov5/releases) TTA |1280 |**55.0** |**55.0** |**72.0** |70.8 | |-  |-\n",
        "\n",
        "<details>\n",
        "  <summary>Table Notes (click to expand)</summary>\n",
        "\n",
        "  * AP<sup>test</sup> denotes COCO [test-dev2017](http://cocodataset.org/#upload) server results, all other AP results denote val2017 accuracy.\n",
        "  * AP values are for single-model single-scale unless otherwise noted. **Reproduce mAP** by `python test.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65`\n",
        "  * Speed<sub>GPU</sub> averaged over 5000 COCO val2017 images using a GCP [n1-standard-16](https://cloud.google.com/compute/docs/machine-types#n1_standard_machine_types) V100 instance, and includes FP16 inference, postprocessing and NMS. **Reproduce speed** by `python test.py --data coco.yaml --img 640 --conf 0.25 --iou 0.45`\n",
        "  * All checkpoints are trained to 300 epochs with default settings and hyperparameters (no autoaugmentation).\n",
        "  * Test Time Augmentation ([TTA](https://github.com/ultralytics/yolov5/issues/303)) includes reflection and scale augmentation. **Reproduce TTA** by `python test.py --data coco.yaml --img 1536 --iou 0.7 --augment`\n",
        "\n",
        "</details>\n",
        "\n",
        "<p align=\"left\"><img width=\"800\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/model_plot.png\"></p>\n",
        "\n",
        "<details>\n",
        "  <summary>Figure Notes (click to expand)</summary>\n",
        "\n",
        "  * GPU Speed measures end-to-end time per image averaged over 5000 COCO val2017 images using a V100 GPU with batch size 32, and includes image preprocessing, PyTorch FP16 inference, postprocessing and NMS.\n",
        "  * EfficientDet data from [google/automl](https://github.com/google/automl) at batch size 8.\n",
        "  * **Reproduce** by `python test.py --task study --data coco.yaml --iou 0.7 --weights yolov5s6.pt yolov5m6.pt yolov5l6.pt yolov5x6.pt`\n",
        "\n",
        "</details>\n",
        "\n",
        "## Load From PyTorch Hub\n",
        "\n",
        "\n",
        "This example loads a pretrained **YOLOv5s** model and passes an image for inference. YOLOv5 accepts **URL**, **Filename**, **PIL**, **OpenCV**, **Numpy** and **PyTorch** inputs, and returns detections in **torch**, **pandas**, and **JSON** output formats. See our [YOLOv5 PyTorch Hub Tutorial](https://github.com/ultralytics/yolov5/issues/36) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "chemical-brook",
      "metadata": {
        "id": "chemical-brook",
        "outputId": "5630d204-baf6-4e9e-cf6c-f0d0a0f89964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 üöÄ 2022-5-8 torch 1.11.0+cu113 CUDA:0 (Tesla K80, 11441MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients\n",
            "Adding AutoShape... \n",
            "Saved 1 image to \u001b[1mruns/detect/exp6\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          xmin        ymin         xmax        ymax  confidence  class    name\n",
              "0   746.699036   41.938354  1145.124268  710.435425    0.943767      0  person\n",
              "1   116.114258  199.385376  1066.704224  711.959290    0.918710      0  person\n",
              "2   433.970581  434.957611   524.817871  717.941650    0.882575     27     tie\n",
              "3  1091.592651  325.928833  1279.609985  717.494995    0.374365      0  person\n",
              "4   990.587585  308.062164  1035.410889  413.162933    0.364092     27     tie"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fcf3e95f-d413-4b1c-87fc-2828b9aaab49\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>confidence</th>\n",
              "      <th>class</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>746.699036</td>\n",
              "      <td>41.938354</td>\n",
              "      <td>1145.124268</td>\n",
              "      <td>710.435425</td>\n",
              "      <td>0.943767</td>\n",
              "      <td>0</td>\n",
              "      <td>person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>116.114258</td>\n",
              "      <td>199.385376</td>\n",
              "      <td>1066.704224</td>\n",
              "      <td>711.959290</td>\n",
              "      <td>0.918710</td>\n",
              "      <td>0</td>\n",
              "      <td>person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>433.970581</td>\n",
              "      <td>434.957611</td>\n",
              "      <td>524.817871</td>\n",
              "      <td>717.941650</td>\n",
              "      <td>0.882575</td>\n",
              "      <td>27</td>\n",
              "      <td>tie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1091.592651</td>\n",
              "      <td>325.928833</td>\n",
              "      <td>1279.609985</td>\n",
              "      <td>717.494995</td>\n",
              "      <td>0.374365</td>\n",
              "      <td>0</td>\n",
              "      <td>person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>990.587585</td>\n",
              "      <td>308.062164</td>\n",
              "      <td>1035.410889</td>\n",
              "      <td>413.162933</td>\n",
              "      <td>0.364092</td>\n",
              "      <td>27</td>\n",
              "      <td>tie</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fcf3e95f-d413-4b1c-87fc-2828b9aaab49')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fcf3e95f-d413-4b1c-87fc-2828b9aaab49 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fcf3e95f-d413-4b1c-87fc-2828b9aaab49');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5x', pretrained=True)\n",
        "\n",
        "# Images\n",
        "imgs = ['https://ultralytics.com/images/zidane.jpg']  # batch of images\n",
        "\n",
        "# Inference\n",
        "results = model(imgs)\n",
        "\n",
        "\n",
        "# Results\n",
        "#results.print()\n",
        "results.save()  # or .show()\n",
        "\n",
        "results.xyxy[0]  # img1 predictions (tensor)\n",
        "results.pandas().xyxy[0]  # img1 predictions (pandas)\n",
        "#      xmin    ymin    xmax   ymax  confidence  class    name\n",
        "# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\n",
        "# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\n",
        "# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\n",
        "# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "potential-instrument",
      "metadata": {
        "id": "potential-instrument"
      },
      "source": [
        "## Citation\n",
        "\n",
        "[![DOI](https://zenodo.org/badge/264818686.svg)](https://zenodo.org/badge/latestdoi/264818686)\n",
        "\n",
        "\n",
        "## Contact\n",
        "\n",
        "\n",
        "**Issues should be raised directly in https://github.com/ultralytics/yolov5.** For business inquiries or professional support requests please visit [https://ultralytics.com](https://ultralytics.com) or email Glenn Jocher at [glenn.jocher@ultralytics.com](mailto:glenn.jocher@ultralytics.com).\n",
        "\n",
        "\n",
        "&nbsp;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imutils.video import FileVideoStream\n",
        "from imutils.video import FPS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pickle\n",
        "\n",
        "res_name = path+input_name+'.playersbox.yolo'\n",
        "\n",
        "df_segments = pd.read_csv(path+input_name+'.points.csv', sep=';')\n",
        "\n",
        "segments = []\n",
        "for index, row in df_segments.iterrows():\n",
        "  segments.append(range(row['ini'], row['fin']+1))\n",
        "print(segments)\n",
        "# YOLO Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5x', pretrained=True, _verbose=False)\n",
        "model.cuda()\n",
        "model.conf = 0.25  # NMS confidence threshold\n",
        "model.classes = [0]  # (optional list) filter by class, i.e. = [0, 38] for COCO persons, tennis raquet\n",
        "model.max_det = 50  # maximum number of detections per image\n",
        "     \n",
        "\n",
        "video_path = path+input_name+'.mp4'\n",
        "\n",
        "out_video_root = path+'vis_'+input_name+'.mp4'\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "#videoWriter = cv2.VideoWriter(out_video_root, fourcc, fps, size)\n",
        "cap.release()\n",
        "\n",
        "print(\"Analisis of \"+video_path+ \" \"+str(size)+\"x\"+str(fps))\n",
        "\n",
        "print(\"[INFO] starting video file thread...\")\n",
        "fvs = FileVideoStream(video_path).start()\n",
        "time.sleep(1.0)\n",
        "# start the FPS timer\n",
        "fps = FPS().start()\n",
        "i = 0\n",
        "\n",
        "result_box = {}\n",
        "frame_list = []\n",
        "# loop over frames from the video file stream\n",
        "while fvs.more():\n",
        "  frame = fvs.read()\n",
        "  if any([i in frag for frag in segments]):\n",
        "    results = model(frame)\n",
        "    #results.save()\n",
        "    res_list = results.pandas().xyxy[0].values.tolist()\n",
        "    result_box[i]  = [np.array(r[:-2]) for r in res_list]\n",
        "      \n",
        "    if not i%200:\n",
        "      with open(res_name, 'wb') as fp:\n",
        "        pickle.dump(result_box, fp)\n",
        "        print(str(i)+\": players position exporte to \"+str(res_name))\n",
        "  i += 1\n",
        "  #videoWriter.write(frame)\n",
        "  if fvs.Q.qsize() < 2:  # If we are low on frames, give time to producer\n",
        "    time.sleep(0.001)  # Ensures producer runs now, so 2 is sufficient\n",
        "  fps.update()\n",
        "\n",
        "# stop the timer and display FPS information\n",
        "fps.stop()\n",
        "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
        "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
        "\n",
        "fvs.stop()\n",
        "\n",
        "with open(res_name, 'wb') as fp:\n",
        "    pickle.dump(result_box, fp)\n",
        "    \n",
        "#videoWriter.release()"
      ],
      "metadata": {
        "id": "5E0YwpLwIYpW",
        "outputId": "70d286d7-c2fa-43b1-c2ee-1b1fe9827a78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5E0YwpLwIYpW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analisis of /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.mp4 (1280, 720)x30.0\n",
            "[INFO] starting video file thread...\n",
            "200: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "800: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "1000: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "1200: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "1600: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "1800: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "2000: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "3400: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "3600: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "4200: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "5400: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "5600: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "7200: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "7600: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "7800: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n",
            "8600: players position exporte to /content/gdrive/MyDrive/PadelData/Final_PremierQatar_Tie.playersbox.yolo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf runs/detect/*"
      ],
      "metadata": {
        "id": "AQgr0_-2MQVp"
      },
      "id": "AQgr0_-2MQVp",
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "ultralytics_yolov5.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}